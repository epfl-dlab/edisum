"""
Dataset of Wikipedia edit summaries generated by LLM model 
"""
from src.datamodules.abstract import AbstractDataset, AbstractPLDataModule
from src.tools.logger import get_pylogger

from tqdm import tqdm

import os
import csv
import ast
import sys

if __name__ == "__main__":
    log = get_pylogger(__name__, stdout=True)
else:
    log = get_pylogger(__name__)

class EditSummaryDataset(AbstractDataset):
    """
    General dataset
    """
    def __init__(self, **kwargs):
        super().__init__(kwargs)
        self.use_prefix = kwargs.get("use_prefix", False)
        self.tokenizer = kwargs.get("tokenizer")
        self.data = self._load_data(kwargs['load_dataset_params'])

    def _load_data(self, load_dataset_params):
        csv.field_size_limit(sys.maxsize)
        data = []
        data_path = os.path.join(load_dataset_params["data_dir"], f"{load_dataset_params['split']}.csv")
        with open(data_path, "r") as csv_file:
            csv_reader = csv.reader(csv_file, delimiter=',')
            prev_texts_idx, cur_texts_idx, summary_idx = None, None, None
            for i, row in enumerate(tqdm(csv_reader)):
                obj = {}
                if i == 0:
                    summary_idx = row.index('summary')
                    prev_texts_idx = row.index('prev_texts')
                    cur_texts_idx = row.index('cur_texts')
                    continue # jump header
                obj['id'] = int(i)
                obj["summary"] = row[summary_idx]
                prev_texts = row[prev_texts_idx]
                cur_texts = row[cur_texts_idx]
                if self.use_prefix:
                    prefix = "Generate a edit summary for editting text1 to text2: "
                    obj["text"] = prefix + f"text1:{prev_texts}" + f"text2:{cur_texts}"
                else:
                    prev_texts_sep = "<sent_sep>".join(ast.literal_eval(prev_texts))
                    cur_texts_sep = "<sent_sep>".join(ast.literal_eval(cur_texts))
                    obj["text"] = f"<old_text>{prev_texts_sep}<new_text>{cur_texts_sep}"

                # HACK add filter here when needed
                if not load_dataset_params["filter_on_num_tokens"] or self._are_num_tokens_within_bounds(obj):
                    data.append(obj)
                else:
                    continue
                if self.params.get("debug", False) and len(data) >= self.params["debug_k"]:
                    break
        return data
    
    def _are_num_tokens_within_bounds(self, obj):
        # Input tokens
        num_tokens_input = obj.get("num_tokens_input", None)
        if not num_tokens_input:
            try:
                num_tokens_input = len(self.tokenizer(obj["text"])["input_ids"])
            except:
                return False

        if num_tokens_input > self.params["max_num_tokens_input"]:
            return False

        # Target tokens
        num_tokens_target = obj.get("num_tokens_target", None)
        if not num_tokens_target:
            num_tokens_target = len(self.tokenizer(obj["summary"])["input_ids"])

        if num_tokens_target > self.params["max_num_tokens_target"]:
            return False

        # Both within bounds
        return True
    
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        obj = self.data[idx]
        
        return {"id": obj["id"], "text": obj["text"], "target": obj["summary"]}
                
class EditSummaryDataMudule(AbstractPLDataModule):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)