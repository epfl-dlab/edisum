# @package _global_

defaults:
  - override /datamodule: _default
  - override /model: long_t5_large
  - override /trainer: ddp # [single_gpu, cpu]

trainer:
#  devices: [0]  # Number of devices to train on (int), which devices to train on (list or str), or "auto".
  max_steps: -1
  max_epochs: 10
  devices: 2
  accumulate_grad_batches: 1
  val_check_interval: 10000

test: true
dir: ???
datamodule:
  data_dir: ${dir}
  use_prefix: false
  batch_size: 2
  num_workers: 8
  debug: false
  debug_k: 12
  max_num_tokens_input: 1024
  max_num_tokens_target: 1024


callback:
  model_checkpoint:
    save_top_k: 1

model:
  optimizer:
    lr: 3.0e-04
    adam_eps: 1.0e-08
    weight_decay: 0.05
  scheduler:
    name: polynomial
    lr_end: 3.0e-05
    warmup_updates: 1000
    total_num_updates: ${trainer.max_steps}

run_name: ???
