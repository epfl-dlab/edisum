# @package _global_

defaults:
  - override /datamodule: _default
  - override /model: long_t5_large
  - override /trainer: single_gpu # [single_gpu, cpu]

dir: ???
checkpoint_path: ???

logs_subfolder: inference
trainer:
  devices: [1]  # Number of devices to train on (int), which devices to train on (list or str), or "auto".
  max_steps: -1
  max_epochs: 5

test: true
resume_from_checkpoint: ${checkpoint_path}

dataset_parameters:
  train: null
  val: null

datamodule:
  data_dir: ${dir}
  use_prefix: false
  batch_size: 2
  num_workers: 8
  debug: false
  debug_k: 12
  max_num_tokens_input: 1024
  max_num_tokens_target: 1024

callback:
  model_checkpoint:
    save_top_k: 1

model:
  optimizer:
    lr: 3.0e-04
    adam_eps: 1.0e-08
    weight_decay: 0.05
  scheduler:
    name: polynomial
    lr_end: 3.0e-05
    warmup_updates: 1000
    total_num_updates: ${trainer.max_steps}

run_name: ???
